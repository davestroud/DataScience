---
title: "WebScrapingGoogle"
author: "David Stroud"
date: "3/6/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

---
title: "WebScraping"
author: "David Stroud"
date: "3/6/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
 # General-purpose data wrangling
    library(tidyverse)  
    # Parsing of HTML/XML files  
    library(rvest)    
    # String manipulation
    library(stringr)   
    # Verbose regular expressions
    library(rebus)     
    # Eases DateTime manipulation
    library(lubridate)
```

```{r}
# Landing page for Google Review
 url <-'https://www.trustpilot.com/review/www.google.com'
```

```{r}
 get_last_page <- function(html){

      pages_data <- html %>% 
                      # The '.' indicates the class
                      html_nodes('.pagination-page') %>% 
                      # Extract the raw text as a list
                      html_text()                   

      # The second to last of the buttons is the one
      pages_data[(length(pages_data)-1)] %>%            
        # Take the raw string
        unname() %>%                                     
        # Convert to number
        as.numeric()                                     
    }
```

```{r}
 first_page <- read_html(url)
    (latest_page_number <- get_last_page(first_page))
```

```{r}
# Generate a list of all relevant html's
list_of_pages <- str_c(url, '?page=', 1:latest_page_number)
```

```{r}
 get_reviews <- function(html){
      html %>% 
        # The relevant tag
        html_nodes('.review-info__body__text') %>%      
        html_text() %>% 
        # Trim additional white space
        str_trim() %>%                       
        # Convert the list into a vector
        unlist()                             
    }

    get_reviewer_names <- function(html){
      html %>% 
        html_nodes('.consumer-info__details__name') %>% 
        html_text() %>% 
        str_trim() %>% 
        unlist()
    }
```


```{r}
get_review_dates <- function(html){

      status <- html %>% 
                  html_nodes('time') %>% 
                  # The status information is this time a tag attribute
                  html_attrs() %>%             
                  # Extract the second element
                  map(2) %>%                    
                  unlist() 

      dates <- html %>% 
                  html_nodes('time') %>% 
                  html_attrs() %>% 
                  map(1) %>% 
                  # Parse the string into a datetime object with lubridate
                  ymd_hms() %>%                 
                  unlist()

     # Combine the status and the date information to filter one via the other
      return_dates <- tibble(status = status, dates = dates) %>%   
                        # Only these are actual reviews
                        filter(status == 'ndate') %>%              
                        # Select and convert to vector
                        pull(dates) %>%                            
                        # Convert DateTimes to POSIX objects
                        as.POSIXct(origin = '1970-01-01 00:00:00') 

      # The lengths still occasionally do not line up. You then arbitrarily crop the dates to fit
      # This can cause data imperfections, however reviews on one page are generally close in time)

      length_reviews <- length(get_reviews(html))

       return_reviews <- if (length(return_dates)> length_reviews){
          return_dates[1:length_reviews]
        } else{
          return_dates
        }
      return_reviews
    }
```

```{r}
get_star_rating <- function(html){

      # The pattern you look for: the first digit after `count-`
      pattern = 'count-'%R% capture(DIGIT)    

      vector <-  html %>% 
        html_nodes("[itemprop=reviewRating]") %>% 
         html_attr('class') %>%  
         str_match(pattern) %>% 
         map(1) %>%  
         as.numeric()  
       vector <-  vector[!is.na(vector)]
       vector
    }
```


```{r}
get_data_table <- function(html, company_name){

      # Extract the Basic information from the HTML
      reviews <- get_reviews(html)
      reviewer_names <- get_reviewer_names(html)
      dates <- get_review_dates(html)
      ratings <- get_star_rating(html)

      # Combine into a tibble
      combined_data <- tibble(reviewer = reviewer_names,
                              date = dates,
                              rating = ratings,
                              review = reviews) 

      # Tag the individual data with the company name
      combined_data %>% 
        mutate(company = company_name) %>% 
        select(company, reviewer, date, rating, review)
    }
```


```{r}
# Wrap in a fuction that extracts the HTML from URL, such that handling is more convienient
get_data_from_url <- function(url, company_name){
      html <- read_html(url)
      get_data_table(html, company_name)
    }
```


```{r}
scrape_write_table <- function(url, company_name){

      # Read first page
      first_page <- read_html(url)

      # Extract the number of pages that have to be queried
      latest_page_number <- get_last_page(first_page)

      # Generate the target URLs
      list_of_pages <- str_c(url, '?page=', 1:latest_page_number)
      
      # Apply the extraction and bind the individual results back into one table, 
      # which is then written as a tsv file into the working directory
      list_of_pages %>% 
        # Apply to all URLs
        map(get_data_from_url, company_name) %>%  
        # Combine the tibbles into one tibble
        bind_rows() %>%                           
        # Write a tab-separated file
        write_tsv(str_c(company_name,'.tsv'))     
    }
```


```{r}
 scrape_write_table(url, 'google')

    goog_tbl <- read_tsv('google.tsv')
    tail(goog_tbl, 5)
```


```{r}
# For working with time series
    library(xts)      

    # For hypothesis testing
    library(infer)
```



```{r}
amazon <- read_tsv('amazon.tsv')
google <- read_tsv('google.tsv')
```

```{r}
full_data <- rbind(amazon, google)

    full_data%>%
      group_by(company) %>% 
      summarise(count = n(), mean_rating = mean(rating))
```


```{r}
amazon_ts <- xts(amazon$rating, amazon$date)
colnames(amazon_ts) <- 'rating'
google_ts <- xts(google$rating, google$date)
colnames(google_ts) <- 'rating'

open_ended_interval <- '2016-01-01/'

# Subsetting the time series
amazon_sts <- amazon_ts[open_ended_interval] 
google_sts <- google_ts[open_ended_interval]
```

```{r}
amazon_month_avg <-  apply.monthly(amazon_sts, colMeans, na.rm = T)
amazon_month_count  <-  apply.monthly(amazon_sts, FUN = length)

google_month_avg <-  apply.monthly(google_sts, colMeans, na.rm = T)
google_month_count  <-  apply.monthly(google_sts, FUN = length)
```


```{r}
  start_date = '2016-01-01/'
    full_data <- full_data %>%  
      filter(date >= start_date) %>% 
      mutate(weekday = weekdays(date, abbreviate = T),
             hour = hour(date))

    # Treat the weekdays as factor. 
    # The order is for the plotting only
    full_data$weekday <-  factor(full_data$weekday, 
                                 levels = c('Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'))
```

```{r}
 hypothesis_data <- full_data %>% 
      mutate(is_weekend = ifelse(weekday %in% c('Sat', 'Sun'), 1, 0)) %>% 
      select(company, is_weekend, rating)

hypothesis_data %>% 
      group_by(company, is_weekend) %>% 
      summarise(avg_rating = mean(rating)) %>% 
      spread(key = is_weekend, value = avg_rating) %>% 
      rename(weekday = '0', weekend = '1')
```




